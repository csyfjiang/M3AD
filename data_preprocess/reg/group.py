CUSTOM_TEMP_DIR = 'F:/temp'
os.makedirs(CUSTOM_TEMP_DIR, exist_ok=True)


os.environ['TMPDIR'] = CUSTOM_TEMP_DIR      # Linux/Unix
os.environ['TMP'] = CUSTOM_TEMP_DIR         # Windows
os.environ['TEMP'] = CUSTOM_TEMP_DIR        # Windows
os.environ['TEMPDIR'] = CUSTOM_TEMP_DIR     # å¤‡ç”¨


tempfile.tempdir = CUSTOM_TEMP_DIR

# print(f"  Custom temporary directory set to: {CUSTOM_TEMP_DIR}")
# print(f"  Python tempfile directory: {tempfile.gettempdir()}")

import sys
import torch
import nibabel as nib
import numpy as np
import pandas as pd
from pathlib import Path
import time
import gc
import ants
import warnings
import glob
from datetime import datetime, timedelta
import traceback
from tqdm import tqdm
import json

warnings.filterwarnings('ignore')

# å¯¼å…¥ç”¨äºæ–¹å‘æ ¡æ­£çš„åº“
from monai.transforms import Orientation
from monai.data import MetaTensor

# å¦‚æœtorchä¸­æ²¡æœ‰GradScalerï¼Œä»æ­£ç¡®ä½ç½®å¯¼å…¥
if not hasattr(torch, 'GradScaler'):
    try:
        from torch.cuda.amp import GradScaler
        torch.GradScaler = GradScaler
        # print("GradScaler patched successfully")
    except ImportError:
        print("Unable to patch GradScaler, please update PyTorch")

# ç°åœ¨ç»§ç»­HD-BETåˆå§‹åŒ–
from HD_BET.hd_bet_prediction import get_hdbet_predictor, hdbet_predict
from HD_BET.checkpoint_download import maybe_download_parameters


def setup_directories(base_output_dir):
    """
    è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„
    """
    brain_mask_dir = os.path.join(base_output_dir, 'brain_mask')
    reg_dir = os.path.join(base_output_dir, 'reg')
    progress_dir = os.path.join(base_output_dir, 'progress_logs')
    temp_dir = os.path.join(base_output_dir, 'temp_processing')  # ä¸“ç”¨ä¸´æ—¶ç›®å½•

    os.makedirs(brain_mask_dir, exist_ok=True)
    os.makedirs(reg_dir, exist_ok=True)
    os.makedirs(progress_dir, exist_ok=True)
    os.makedirs(temp_dir, exist_ok=True)

    print(f"âœ… Output directories created:")
    print(f"   Brain mask: {brain_mask_dir}")
    print(f"   Registration: {reg_dir}")
    print(f"   Progress logs: {progress_dir}")
    print(f"   Processing temp: {temp_dir}")

    return brain_mask_dir, reg_dir, progress_dir, temp_dir


def load_csv_file_list(csv_file):
    """
    ä»CSVæ–‡ä»¶ä¸­åŠ è½½æ–‡ä»¶åˆ—è¡¨

    Parameters:
    -----------
    csv_file : str
        CSVæ–‡ä»¶è·¯å¾„

    Returns:
    --------
    pd.DataFrame : åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„DataFrame
    """
    try:
        df = pd.read_csv(csv_file)
        print(f"âœ… Loaded CSV: {csv_file}")
        print(f"   Records: {len(df)}")

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['full_path', 'filename']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            print(f"âŒ Missing required columns: {missing_columns}")
            return None

        # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
        existing_files = df[df['full_path'].apply(os.path.exists)]
        missing_count = len(df) - len(existing_files)

        if missing_count > 0:
            print(f"âš ï¸  Warning: {missing_count} files not found on disk")

        print(f"ğŸ“ Valid files to process: {len(existing_files)}")
        return existing_files

    except Exception as e:
        print(f"âŒ Error loading CSV {csv_file}: {e}")
        return None


def save_batch_progress(progress_file, batch_info, processed_files, failed_files):
    """
    ä¿å­˜æ‰¹æ¬¡å¤„ç†è¿›åº¦
    """
    progress_data = {
        'batch_info': batch_info,
        'processed_files': processed_files,
        'failed_files': failed_files,
        'timestamp': datetime.now().isoformat(),
        'total_processed': len(processed_files),
        'total_failed': len(failed_files)
    }

    with open(progress_file, 'w') as f:
        json.dump(progress_data, f, indent=2)


def load_batch_progress(progress_file):
    """
    åŠ è½½æ‰¹æ¬¡å¤„ç†è¿›åº¦
    """
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r') as f:
                return json.load(f)
        except:
            return None
    return None


def process_single_nii_file(input_file, brain_mask_dir, reg_dir, temp_dir, predictor=None, pbar=None):
    """
    å¤„ç†å•ä¸ªNIIæ–‡ä»¶çš„å®Œæ•´æµç¨‹

    Parameters:
    -----------
    input_file : str
        è¾“å…¥æ–‡ä»¶è·¯å¾„
    brain_mask_dir : str
        è„‘åŒºæå–ç»“æœä¿å­˜ç›®å½•
    reg_dir : str
        é…å‡†ç»“æœä¿å­˜ç›®å½•
    temp_dir : str
        ä¸“ç”¨ä¸´æ—¶æ–‡ä»¶ç›®å½•
    predictor : object
        HD-BETé¢„æµ‹å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºå¤ç”¨ï¼‰
    pbar : tqdm
        è¿›åº¦æ¡å¯¹è±¡
    """
    # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    filename = os.path.basename(input_file)
    base_name = os.path.splitext(filename)[0]  # å»æ‰.nii

    # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶åï¼ˆé¿å…å¹¶å‘å†²çªï¼‰
    unique_id = f"{base_name}_{int(time.time() * 1000000) % 1000000}"

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
    brain_mask_output = os.path.join(brain_mask_dir, f"{base_name}_brain_mask.nii.gz")
    reg_output = os.path.join(reg_dir, f"{base_name}_reg.nii.gz")

    # è®¾ç½®ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨ä¸“ç”¨ä¸´æ—¶ç›®å½•ï¼‰
    temp_ras_file = os.path.join(temp_dir, f"{unique_id}_temp_ras.nii")
    temp_n4_file = os.path.join(temp_dir, f"{unique_id}_temp_n4.nii.gz")
    temp_brain_file = os.path.join(temp_dir, f"{unique_id}_temp_brain.nii.gz")

    try:
        # æ­¥éª¤1: æ–¹å‘æ ¡æ­£
        if pbar:
            pbar.set_postfix_str(f"Orientation correction: {filename}")

        # è¯»å–åŸå§‹å›¾åƒ
        img = nib.load(input_file)
        data = img.get_fdata()

        # è·å–åŸå§‹æ–¹å‘
        original_orientation = nib.aff2axcodes(img.affine)

        # æ–¹å‘æ ¡æ­£ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if original_orientation != ('R', 'A', 'S'):
            data_tensor = torch.from_numpy(data).unsqueeze(0)
            meta_data = MetaTensor(data_tensor, affine=img.affine)
            orient = Orientation(axcodes="RAS")
            transformed = orient(meta_data)
            data_ras = transformed.numpy()[0]
            affine_ras = transformed.affine

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            img_ras = nib.Nifti1Image(data_ras, affine_ras)
            nib.save(img_ras, temp_ras_file)
            working_file = temp_ras_file
        else:
            working_file = input_file

        # æ­¥éª¤2: è¯»å–ANTså›¾åƒ
        if pbar:
            pbar.set_postfix_str(f"Reading with ANTs: {filename}")

        original_img = ants.image_read(working_file)

        # æ­¥éª¤3: N4åç½®åœºæ ¡æ­£
        if pbar:
            pbar.set_postfix_str(f"N4 bias correction: {filename}")

        corrected_img = ants.n4_bias_field_correction(
            original_img,
            mask=None,
            shrink_factor=4,
            convergence={'iters': [50, 50, 50, 50], 'tol': 1e-7},
            spline_param=200
        )

        # æ­¥éª¤4: HD-BETè„‘åŒºæå–
        if pbar:
            pbar.set_postfix_str(f"Brain extraction: {filename}")

        # ä¿å­˜N4æ ¡æ­£åçš„ä¸´æ—¶æ–‡ä»¶
        ants.image_write(corrected_img, temp_n4_file)

        try:
            # è®¾ç½®è®¾å¤‡
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            # å¦‚æœæ²¡æœ‰æä¾›é¢„æµ‹å™¨ï¼Œåˆ›å»ºæ–°çš„
            if predictor is None:
                predictor = get_hdbet_predictor(
                    use_tta=False,
                    device=device,
                    verbose=False
                )

            # æ‰§è¡Œbrain extraction
            hdbet_predict(
                input_file_or_folder=temp_n4_file,
                output_file_or_folder=temp_brain_file,
                predictor=predictor,
                keep_brain_mask=True,
                compute_brain_extracted_image=True
            )

            # è¯»å–ç»“æœ
            brain_img = ants.image_read(temp_brain_file)

            # ä¿å­˜è„‘åŒºæå–ç»“æœ
            ants.image_write(brain_img, brain_mask_output)
            brain_extraction_success = True

        except Exception as e:
            # ä½¿ç”¨ANTsPyå¤‡ç”¨æ–¹æ³•
            try:
                brain_extraction_result = ants.brain_extraction(corrected_img, modality="t1")
                brain_img = brain_extraction_result['brain_image']
                ants.image_write(brain_img, brain_mask_output)
                brain_extraction_success = True
            except Exception as e2:
                brain_img = corrected_img
                ants.image_write(brain_img, brain_mask_output)
                brain_extraction_success = False

        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # æ­¥éª¤5: é…å‡†åˆ°MNIç©ºé—´
        if pbar:
            pbar.set_postfix_str(f"Registration: {filename}")

        try:
            # è·å–MNIæ¨¡æ¿
            template = ants.image_read(ants.get_ants_data('mni'))

            registration_result = ants.registration(
                fixed=template,
                moving=brain_img,
                type_of_transform='SyN',
                verbose=False
            )

            registered_img = registration_result['warpedmovout']
            ants.image_write(registered_img, reg_output)
            registration_success = True

        except Exception as e:
            ants.image_write(brain_img, reg_output)
            registration_success = False

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨æ–°çš„è·¯å¾„ï¼‰
        temp_files = [temp_ras_file, temp_n4_file, temp_brain_file]

        for temp_file in temp_files:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    if pbar:
                        pbar.set_postfix_str(f"Cleaned temp: {os.path.basename(temp_file)}")
                except:
                    pass

        # æ¸…ç†å†…å­˜
        gc.collect()

        return {
            'status': 'success',
            'brain_extraction_success': brain_extraction_success,
            'registration_success': registration_success,
            'brain_mask_file': brain_mask_output,
            'reg_file': reg_output
        }

    except Exception as e:
        # å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = [temp_ras_file, temp_n4_file, temp_brain_file]
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass

        return {
            'status': 'failed',
            'error': str(e),
            'brain_extraction_success': False,
            'registration_success': False,
            'brain_mask_file': None,
            'reg_file': None
        }


def process_csv_batch(csv_file, brain_mask_dir, reg_dir, progress_dir, temp_dir, predictor=None, resume=False):
    """
    å¤„ç†å•ä¸ªCSVæ‰¹æ¬¡

    Parameters:
    -----------
    csv_file : str
        CSVæ–‡ä»¶è·¯å¾„
    brain_mask_dir : str
        è„‘åŒºæå–ç»“æœä¿å­˜ç›®å½•
    reg_dir : str
        é…å‡†ç»“æœä¿å­˜ç›®å½•
    progress_dir : str
        è¿›åº¦è®°å½•ç›®å½•
    predictor : object
        HD-BETé¢„æµ‹å™¨
    resume : bool
        æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­

    Returns:
    --------
    dict : æ‰¹æ¬¡å¤„ç†ç»“æœ
    """

    csv_basename = os.path.splitext(os.path.basename(csv_file))[0]
    progress_file = os.path.join(progress_dir, f"{csv_basename}_progress.json")

    print(f"\nğŸ”„ Processing CSV batch: {csv_basename}")
    print(f"   CSV file: {csv_file}")
    print(f"   Progress file: {progress_file}")

    # åŠ è½½CSVæ–‡ä»¶
    df = load_csv_file_list(csv_file)
    if df is None or df.empty:
        return {
            'csv_file': csv_file,
            'status': 'failed',
            'error': 'Failed to load CSV or no valid files',
            'total_files': 0,
            'processed': 0,
            'failed': 0
        }

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è¿›åº¦
    processed_files = []
    failed_files = []
    start_index = 0

    if resume:
        progress_data = load_batch_progress(progress_file)
        if progress_data:
            processed_files = progress_data.get('processed_files', [])
            failed_files = progress_data.get('failed_files', [])
            start_index = len(processed_files) + len(failed_files)
            print(f"ğŸ“‹ Resuming from index {start_index} (processed: {len(processed_files)}, failed: {len(failed_files)})")

    # è·å–å¾…å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    files_to_process = df.iloc[start_index:].copy()

    if files_to_process.empty:
        print(f"âœ… All files in {csv_basename} already processed")
        return {
            'csv_file': csv_file,
            'status': 'completed',
            'total_files': len(df),
            'processed': len(processed_files),
            'failed': len(failed_files),
            'skipped': 0
        }

    print(f"ğŸ“Š Batch statistics:")
    print(f"   Total files in CSV: {len(df)}")
    print(f"   Already processed: {len(processed_files)}")
    print(f"   Already failed: {len(failed_files)}")
    print(f"   Remaining to process: {len(files_to_process)}")

    # å¤„ç†æ–‡ä»¶
    batch_start_time = time.time()

    with tqdm(total=len(files_to_process),
              desc=f"ğŸ§  Processing {csv_basename}",
              unit="file",
              ncols=120) as pbar:

        for idx, (_, row) in enumerate(files_to_process.iterrows()):
            file_path = row['full_path']
            filename = row['filename']

            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file_path):
                    failed_files.append({
                        'file': file_path,
                        'error': 'File not found',
                        'timestamp': datetime.now().isoformat()
                    })
                    pbar.set_postfix_str(f"Skipped: {filename} (not found)")
                    pbar.update(1)
                    continue

                # å¤„ç†æ–‡ä»¶
                result = process_single_nii_file(file_path, brain_mask_dir, reg_dir, temp_dir, predictor, pbar)

                if result['status'] == 'success':
                    processed_files.append({
                        'file': file_path,
                        'filename': filename,
                        'brain_extraction_success': result['brain_extraction_success'],
                        'registration_success': result['registration_success'],
                        'brain_mask_file': result['brain_mask_file'],
                        'reg_file': result['reg_file'],
                        'timestamp': datetime.now().isoformat()
                    })
                else:
                    failed_files.append({
                        'file': file_path,
                        'filename': filename,
                        'error': result['error'],
                        'timestamp': datetime.now().isoformat()
                    })

                # æ¯10ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
                if (idx + 1) % 10 == 0:
                    save_batch_progress(progress_file, {
                        'csv_file': csv_file,
                        'csv_basename': csv_basename,
                        'total_files': len(df),
                        'start_time': batch_start_time
                    }, processed_files, failed_files)

                    # æ¸…ç†å†…å­˜
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                # æ›´æ–°è¿›åº¦æ¡
                success_rate = len(processed_files) / (len(processed_files) + len(failed_files)) * 100 if (len(processed_files) + len(failed_files)) > 0 else 0
                pbar.set_postfix_str(f"Success: {success_rate:.1f}%")
                pbar.update(1)

            except Exception as e:
                failed_files.append({
                    'file': file_path,
                    'filename': filename,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                pbar.set_postfix_str(f"Error: {filename}")
                pbar.update(1)

    # ä¿å­˜æœ€ç»ˆè¿›åº¦
    batch_end_time = time.time()
    batch_duration = batch_end_time - batch_start_time

    final_progress = {
        'csv_file': csv_file,
        'csv_basename': csv_basename,
        'total_files': len(df),
        'start_time': batch_start_time,
        'end_time': batch_end_time,
        'duration_seconds': batch_duration,
        'completed': True
    }

    save_batch_progress(progress_file, final_progress, processed_files, failed_files)

    return {
        'csv_file': csv_file,
        'csv_basename': csv_basename,
        'status': 'completed',
        'total_files': len(df),
        'processed': len(processed_files),
        'failed': len(failed_files),
        'duration_minutes': batch_duration / 60
    }


def batch_process_from_csvs(csv_dir, output_dir, resume=False):
    """
    ä»CSVæ–‡ä»¶æ‰¹é‡å¤„ç†ADNIæ•°æ®

    Parameters:
    -----------
    csv_dir : str
        åŒ…å«CSVæ–‡ä»¶çš„ç›®å½•
    output_dir : str
        è¾“å‡ºç›®å½•è·¯å¾„
    resume : bool
        æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­
    """

    print("ğŸš€ ADNI CSV Batch Preprocessing Pipeline")
    print("=" * 80)
    print(f"CSV directory: {csv_dir}")
    print(f"Output directory: {output_dir}")
    print(f"Resume mode: {resume}")
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # æ£€æŸ¥CSVç›®å½•
    if not os.path.exists(csv_dir):
        print(f"âŒ Error: CSV directory does not exist: {csv_dir}")
        return

    # è®¾ç½®è¾“å‡ºç›®å½•
    brain_mask_dir, reg_dir, progress_dir, temp_dir = setup_directories(output_dir)

    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_pattern = os.path.join(csv_dir, "ADNI_CN_*.csv")
    csv_files = sorted(glob.glob(csv_pattern))

    if not csv_files:
        print(f"âŒ No ADNI_CN_*.csv files found in {csv_dir}")
        return

    print(f"ğŸ“ Found {len(csv_files)} CSV files to process:")
    for csv_file in csv_files:
        print(f"   ğŸ“„ {os.path.basename(csv_file)}")

    # åˆå§‹åŒ–HD-BETé¢„æµ‹å™¨
    print("\nğŸ”§ Initializing HD-BET predictor...")
    try:
        maybe_download_parameters()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        predictor = get_hdbet_predictor(
            use_tta=False,
            device=device,
            verbose=False
        )
        print("âœ… HD-BET predictor initialized successfully")
    except Exception as e:
        print(f"âŒ Failed to initialize HD-BET predictor: {e}")
        print("   Will use ANTsPy as backup for all files")
        predictor = None

    # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
    total_start_time = time.time()
    all_results = []

    for csv_idx, csv_file in enumerate(csv_files):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ Processing CSV {csv_idx + 1}/{len(csv_files)}")
        print(f"{'='*60}")

        try:
            result = process_csv_batch(csv_file, brain_mask_dir, reg_dir, progress_dir, temp_dir, predictor, resume)
            all_results.append(result)

            print(f"\nâœ… Completed: {result['csv_basename']}")
            print(f"   Files processed: {result['processed']}")
            print(f"   Files failed: {result['failed']}")
            if 'duration_minutes' in result:
                print(f"   Duration: {result['duration_minutes']:.1f} minutes")

        except Exception as e:
            print(f"âŒ Error processing {csv_file}: {e}")
            all_results.append({
                'csv_file': csv_file,
                'status': 'error',
                'error': str(e)
            })

    # æœ€ç»ˆç»Ÿè®¡
    total_duration = time.time() - total_start_time

    print(f"\n{'='*80}")
    print("ğŸŠ ALL CSV BATCHES COMPLETED!")
    print(f"{'='*80}")
    print(f"ğŸ“Š Final Statistics:")

    total_processed = sum(r.get('processed', 0) for r in all_results)
    total_failed = sum(r.get('failed', 0) for r in all_results)
    total_files = sum(r.get('total_files', 0) for r in all_results)
    successful_batches = sum(1 for r in all_results if r.get('status') == 'completed')

    print(f"   CSV files processed: {successful_batches}/{len(csv_files)}")
    print(f"   Total files: {total_files}")
    print(f"   Successfully processed: {total_processed}")
    print(f"   Failed: {total_failed}")
    print(f"   Success rate: {total_processed/total_files*100:.1f}%" if total_files > 0 else "   Success rate: 0%")
    print(f"")
    print(f"â±ï¸  Total processing time: {total_duration/60:.1f} minutes ({total_duration/3600:.1f} hours)")
    print(f"ğŸ“ Output directories:")
    print(f"   Brain mask results: {brain_mask_dir}")
    print(f"   Registration results: {reg_dir}")
    print(f"   Progress logs: {progress_dir}")
    print("=" * 80)


def main():
    """
    ä¸»å‡½æ•°
    """
    print(f"ğŸ—‚ï¸  éªŒè¯ä¸´æ—¶ç›®å½•è®¾ç½®:")
    print(f"   ç³»ç»ŸTEMPç›®å½•: {os.environ.get('TEMP', 'Not Set')}")
    print(f"   Python tempfileç›®å½•: {tempfile.gettempdir()}")
    print(f"   å¯ç”¨ç©ºé—´æ£€æŸ¥...")

    # æ£€æŸ¥Dç›˜å¯ç”¨ç©ºé—´
    if os.path.exists('D:'):
        import shutil
        total, used, free = shutil.disk_usage('D:')
        print(f"   Dç›˜å¯ç”¨ç©ºé—´: {free // (1024**3)} GB")
        if free < 10 * (1024**3):  # å°‘äº10GB
            print(f"   âš ï¸  è­¦å‘Š: Dç›˜å¯ç”¨ç©ºé—´ä¸è¶³10GBï¼Œå»ºè®®æ¸…ç†ç©ºé—´")

    # è®¾ç½®è·¯å¾„
    csv_dir = r"F:\data\ADNI\CN_csv_batches"  # CSVæ–‡ä»¶ç›®å½•
    output_dir = r"F:\data\ADNI\CN_preprocessed"  # è¾“å‡ºç›®å½•

    # æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ (True/False)
    resume = True

    print(f"\nğŸ“ å¤„ç†ç›®å½•:")
    print(f"   CSVè¾“å…¥ç›®å½•: {csv_dir}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   ä¸´æ—¶æ–‡ä»¶å°†ä¿å­˜åˆ°: {CUSTOM_TEMP_DIR}")
    print(f"   ä¸“ç”¨å¤„ç†ä¸´æ—¶ç›®å½•: {output_dir}/temp_processing")

    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    batch_process_from_csvs(csv_dir, output_dir, resume)

    # æœ€ç»ˆæ¸…ç†
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # æ¸…ç†æ®‹ç•™ä¸´æ—¶æ–‡ä»¶
    try:
        temp_files = glob.glob(os.path.join(CUSTOM_TEMP_DIR, "*"))
        if temp_files:
            print(f"\nğŸ§¹ æ¸…ç†æ®‹ç•™ä¸´æ—¶æ–‡ä»¶: {len(temp_files)} ä¸ª")
            for temp_file in temp_files:
                try:
                    if os.path.isfile(temp_file):
                        os.remove(temp_file)
                except:
                    pass
    except:
        pass

    print("ğŸ‰ All processing completed successfully!")


if __name__ == "__main__":
    main()